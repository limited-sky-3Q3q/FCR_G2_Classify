# 数据缺失值处理方法说明

## 一、处理流程概述

该脚本使用了**两阶段缺失值处理**方法：

1. **第一阶段：样本级过滤** - 删除目标变量缺失的行
2. **第二阶段：特征级清理与填充** - 删除高缺失率列 + 中位数填充

---

## 二、详细处理步骤

### 步骤1：删除目标变量缺失的样本

```python
data = data.dropna(subset=['FCR_G2']).copy()
```

**目的：** 确保所有训练样本都有标签（目标变量）

**处理方式：**
- 检查 `FCR_G2` 列（目标变量）
- 删除所有 `FCR_G2` 为空值（NaN）的行
- 保留标签完整的样本用于模型训练

**原因：**
- 目标变量是模型学习的依据
- 缺失标签的样本无法用于监督学习
- 删除这些样本可以保证训练数据质量

---

### 步骤2：排除非特征列

```python
exclude_cols = ['FCR_G2', 'ID']
feature_cols = [col for col in data.columns if col not in exclude_cols]
data_clean = data[feature_cols].copy()
```

**目的：** 只处理特征列的缺失值

**处理方式：**
- 将目标变量 `FCR_G2` 和ID列排除
- 只对特征列进行缺失值处理

---

### 步骤3：删除全空列

```python
non_null_cols = data_clean.columns[data_clean.notna().any()].tolist()
data_clean = data_clean[non_null_cols]
```

**目的：** 移除完全无信息的特征

**处理方式：**
- 检查每一列是否至少有一个非空值
- 删除所有值都是NaN的列

**原因：**
- 全空列对模型训练没有任何贡献
- 保留这些列会增加数据维度但无益处
- 删除可以降低计算复杂度

---

### 步骤4：删除高缺失率列（>50%）

```python
missing_ratio = data_clean.isna().sum() / len(data_clean)
cols_to_keep = missing_ratio[missing_ratio <= 0.5].index.tolist()
data_clean = data_clean[cols_to_keep]
```

**目的：** 移除缺失率过高的特征

**处理方式：**
- 计算每列的缺失率 = 缺失值数量 / 总样本数
- 删除缺失率 > 50% 的列
- 保留缺失率 ≤ 50% 的列

**原因：**
- 缺失率过高（>50%）的特征信息量太少
- 强行填充可能引入较大偏差
- 删除这些特征可以：
  - 提高数据质量
  - 减少噪声干扰
  - 降低过拟合风险

**示例：**
```
特征A: 100个样本中有60个缺失 → 缺失率60% → 删除
特征B: 100个样本中有30个缺失 → 缺失率30% → 保留并填充
```

---

### 步骤5：中位数填充剩余缺失值

```python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='median')
X = pd.DataFrame(
    imputer.fit_transform(data_clean),
    columns=data_clean.columns,
    index=data_clean.index
)
```

**目的：** 填充保留特征中的缺失值

**处理方式：**
- 使用 `SimpleImputer` 类
- 填充策略：`median`（中位数）
- 对每列单独计算其中位数并填充该列的缺失值
- `fit_transform()` 同时完成：
  1. `fit()`: 计算每列的中位数
  2. `transform()`: 用中位数替换缺失值

**为什么选择中位数？**

| 填充策略 | 优点 | 缺点 | 适用场景 |
|---------|------|------|---------|
| **中位数** | 对异常值不敏感、稳健 | 可能低估波动 | 存在异常值、偏态分布 |
| 均值 | 保持数据整体水平 | 易受异常值影响 | 正态分布 |
| 众数 | 保持数据原始特征 | 可能引入偏差 | 分类变量 |

**中位数在本数据集的优势：**
- ✅ 抗异常值：不受极端值影响
- ✅ 稳健性好：适用于各种分布
- ✅ 适合本场景：心理量表数据常有极端值
- ✅ 保持秩序：不改变数据的相对大小关系

---

## 三、完整流程图

```
原始数据 (CSV)
    ↓
┌─────────────────────────────┐
│ 步骤1: 删除目标变量缺失的行 │
│ dropna(subset=['FCR_G2'])   │
└─────────────────────────────┘
    ↓
筛选特征列（排除ID和FCR_G2）
    ↓
┌─────────────────────────────┐
│ 步骤2: 删除全空列           │
│ data_clean.notna().any()    │
└─────────────────────────────┘
    ↓
┌─────────────────────────────┐
│ 步骤3: 删除高缺失率列       │
│ 缺失率 > 50% 的列删除       │
└─────────────────────────────┘
    ↓
┌─────────────────────────────┐
│ 步骤4: 中位数填充           │
│ SimpleImputer(median)      │
└─────────────────────────────┘
    ↓
完整特征矩阵 X (无缺失值)
```

---

## 四、代码实现细节

### SimpleImputer 工作原理

```python
from sklearn.impute import SimpleImputer

# 创建imputer对象
imputer = SimpleImputer(strategy='median')

# fit: 学习训练数据的统计量（中位数）
imputer.fit(data_clean)

# transform: 填充缺失值
X_filled = imputer.transform(data_clean)

# fit_transform: 一步完成
X_filled = imputer.fit_transform(data_clean)
```

**重要提示：**
- `fit()` 只在训练集上调用一次
- 对测试集或新数据，只调用 `transform()`
- 这样确保使用训练集的中位数来填充新数据

### 保存和加载 Imputer

```python
import joblib

# 保存（包含在model_info中）
joblib.dump(imputer, 'imputer.joblib')

# 加载
imputer_loaded = joblib.load('imputer.joblib')

# 对新数据进行相同的填充
X_new_filled = imputer_loaded.transform(new_data)
```

---

## 五、处理效果示例

### 假设原始数据

| 样本 | 特征A | 特征B | 特征C | FCR_G2 |
|------|-------|-------|-------|--------|
| 1 | 10 | 20 | NaN | 1 |
| 2 | 15 | NaN | 30 | 2 |
| 3 | NaN | 25 | 35 | 1 |
| 4 | 20 | NaN | 40 | NaN |

### 处理步骤

1. **删除目标变量缺失的行**
   - 样本4被删除（FCR_G2为空）

2. **特征缺失率计算**
   - 特征A: 1/3 = 33% → 保留
   - 特征B: 2/3 = 67% → 删除（>50%）
   - 特征C: 1/3 = 33% → 保留

3. **中位数填充**
   - 特征A中位数 = median(10, 15) = 12.5
   - 特征C中位数 = median(30, 35) = 32.5

4. **最终结果**

| 样本 | 特征A | 特征C | FCR_G2 |
|------|-------|-------|--------|
| 1 | 10 | 30 | 1 |
| 2 | 15 | 30 | 2 |
| 3 | 12.5 | 35 | 1 |

---

## 六、优缺点分析

### ✅ 优点

1. **简单高效**
   - 代码简洁
   - 计算速度快
   - 易于理解和维护

2. **稳健性强**
   - 中位数对异常值不敏感
   - 不假设数据分布

3. **可复现性好**
   - Imputer保存后可重复使用
   - 确保训练和测试使用相同填充策略

4. **适合本场景**
   - 心理量表数据常有异常值
   - 样本量适中，填充影响可控

### ⚠️ 缺点

1. **可能引入偏差**
   - 填充值可能不完全准确
   - 可能低估特征的变异性

2. **不考虑特征间关系**
   - 每列独立填充，未利用相关特征信息
   - 对于高度相关的特征，可能有更好方法

3. **不适用于分类变量**
   - 本数据集特征多为连续型/有序型
   - 纯分类变量不适合用中位数

---

## 七、其他可选方案对比

| 方法 | 适用场景 | 复杂度 | 说明 |
|------|---------|--------|------|
| **中位数填充** | 有异常值、偏态分布 | 低 | ✅ 本方案使用 |
| 均值填充 | 正态分布、无异常值 | 低 | 简单但不稳健 |
| KNN填充 | 特征间相关性高 | 中 | 利用邻近样本信息 |
| 多重插补 | 缺失机制复杂 | 高 | 统计学最优，计算复杂 |
| 删除缺失样本 | 缺失样本少 | 低 | 适用于少量缺失 |

---

## 八、使用建议

### 何时使用本方法？

✅ **适合：**
- 缺失率中等到较低（≤50%）
- 特征为连续型或有序型
- 数据存在异常值或偏态分布
- 样本量适中，填充影响可控

❌ **不适合：**
- 缺失率过高（>50%），应考虑删除该特征
- 纯分类变量（名义型）
- 特征间存在强相关，可用KNN填充
- 缺失机制为非随机（MNAR）

### 改进建议

1. **缺失率分析**
   ```python
   missing_ratio = data.isna().sum() / len(data)
   print(missing_ratio.sort_values(ascending=False))
   ```

2. **填充前后对比**
   ```python
   # 填充前
   print("填充前:", data_clean.mean())
   # 填充后
   print("填充后:", X_filled.mean())
   ```

3. **敏感度分析**
   - 尝试不同填充策略（均值、KNN）
   - 比较模型性能变化

---

## 九、相关API文档

### SimpleImputer

**参数：**
```python
SimpleImputer(
    missing_values=nan,    # 要填充的值（默认np.nan）
    strategy='mean',       # 填充策略：'mean', 'median', 'most_frequent', 'constant'
    fill_value=None,       # 当strategy='constant'时使用
    verbose=0,             # 详细程度
    copy=True,             # 是否复制数据
    add_indicator=False    # 是否添加缺失指示器列
)
```

**方法：**
- `fit(X)`: 学习填充统计量
- `transform(X)`: 填充缺失值
- `fit_transform(X)`: 一部完成

**属性：**
- `statistics_`: 每列的填充值（中位数、均值等）

### 参考链接
- sklearn文档: https://scikit-learn.org/stable/modules/impute.html

---

## 十、总结

本脚本采用**两阶段缺失值处理**策略：

1. **过滤阶段**：删除无效样本和高缺失率特征
2. **填充阶段**：用中位数稳健填充剩余缺失值

该方法**简单、稳健、高效**，特别适合本数据集的心理量表特征，在保证数据质量的同时，最大化利用可用信息。

---

**文档版本：** 1.0
**创建日期：** 2026年
**适用模型：** LogisticRegression_E1数据集
**准确率：** 92.56% (8折交叉验证)
